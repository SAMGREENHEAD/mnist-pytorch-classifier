{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Understanding PyTorch through Image Classification\n\nThis practical is an introduction to PyTorch by developing a simple image classification model using the MNIST dataset.\n\nThe goal is to classify handwritten digits (0–9) by:\n- Loading and exploring the dataset\n- Creating a neural network from scratch using PyTorch\n- Training and evaluating the model\n- Testing the model's performance\n\nThe practical will involve the following :\n- The PyTorch workflow: datasets, dataloaders, models, training, and evaluation\n- How neural networks learn using backpropagation and loss optimization\n- How to structure PyTorch projects for real-world use cases\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1.1: Explore the Raw MNIST Dataset\n\nBefore we prepare the data for training, it's important to understand what the MNIST dataset looks like.\nI will load the MNIST dataset from the `digit-recognizer` in kaggle  competition's built-in CSV files.\n\n- `train.csv` \n- Each row has a `label` column and 784 pixel values (one per pixel).\n- I will visualize a few samples to understand the data structure.\n\n\nThis helps build intuition about the input our neural network will learn from.\n","metadata":{}},{"cell_type":"code","source":"# Download the MNIST training set\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:49:35.033523Z","iopub.execute_input":"2025-07-29T09:49:35.033946Z","iopub.status.idle":"2025-07-29T09:49:39.072842Z","shell.execute_reply.started":"2025-07-29T09:49:35.033916Z","shell.execute_reply":"2025-07-29T09:49:39.071934Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Display dataset shape and preview\nprint(f\"Dataset shape: {df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:50:01.590866Z","iopub.execute_input":"2025-07-29T09:50:01.591240Z","iopub.status.idle":"2025-07-29T09:50:01.597281Z","shell.execute_reply.started":"2025-07-29T09:50:01.591213Z","shell.execute_reply":"2025-07-29T09:50:01.596079Z"}},"outputs":[{"name":"stdout","text":"Dataset shape: (42000, 785)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:50:11.286812Z","iopub.execute_input":"2025-07-29T09:50:11.287132Z","iopub.status.idle":"2025-07-29T09:50:11.312771Z","shell.execute_reply.started":"2025-07-29T09:50:11.287109Z","shell.execute_reply":"2025-07-29T09:50:11.311459Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Extract images and labels\nlabels = df['label'].values\nimages = df.drop('label', axis=1).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:50:24.169491Z","iopub.execute_input":"2025-07-29T09:50:24.171083Z","iopub.status.idle":"2025-07-29T09:50:24.315189Z","shell.execute_reply.started":"2025-07-29T09:50:24.171040Z","shell.execute_reply":"2025-07-29T09:50:24.313850Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Visualize the first 6 digits\nplt.figure(figsize=(8, 4))\nfor i in range(6):\n    image = images[i].reshape(28, 28)  # Reshape flat vector to 28x28 image\n    plt.subplot(2, 3, i + 1)\n    plt.imshow(image, cmap='gray')\n    plt.title(f\"Label: {labels[i]}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:50:40.848932Z","iopub.execute_input":"2025-07-29T09:50:40.849305Z","iopub.status.idle":"2025-07-29T09:50:41.256167Z","shell.execute_reply.started":"2025-07-29T09:50:40.849278Z","shell.execute_reply":"2025-07-29T09:50:41.255134Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x400 with 6 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Step 1.2: Convert Data to PyTorch Tensors and Create DataLoaders\n\nTo train a model in PyTorch, we need to:\n- Convert the raw image pixel data from the CSV into `torch.Tensor` format\n- Normalize pixel values to the range [0, 1]\n- Create a PyTorch `TensorDataset` and wrap it in a `DataLoader` for batching and shuffling\n}","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Step 1: Normalize pixel values and convert to float32\nimages = images / 255.0  # Scale from [0, 255] to [0.0, 1.0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:52:32.212581Z","iopub.execute_input":"2025-07-29T09:52:32.213015Z","iopub.status.idle":"2025-07-29T09:52:32.362975Z","shell.execute_reply.started":"2025-07-29T09:52:32.212987Z","shell.execute_reply":"2025-07-29T09:52:32.361552Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"\n# Step 2: Convert to PyTorch tensors\nX = torch.tensor(images, dtype=torch.float32)          # Shape: [N, 784]\ny = torch.tensor(labels, dtype=torch.long)             # Shape: [N]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:53:22.480767Z","iopub.execute_input":"2025-07-29T09:53:22.481232Z","iopub.status.idle":"2025-07-29T09:53:22.583948Z","shell.execute_reply.started":"2025-07-29T09:53:22.481202Z","shell.execute_reply":"2025-07-29T09:53:22.583020Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"X,y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:53:33.316425Z","iopub.execute_input":"2025-07-29T09:53:33.317929Z","iopub.status.idle":"2025-07-29T09:53:33.354128Z","shell.execute_reply.started":"2025-07-29T09:53:33.317883Z","shell.execute_reply":"2025-07-29T09:53:33.353120Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([1, 0, 1,  ..., 7, 6, 9]))"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Step 3: Wrap tensors in a TensorDataset\ndataset = TensorDataset(X, y)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:54:07.226855Z","iopub.execute_input":"2025-07-29T09:54:07.227242Z","iopub.status.idle":"2025-07-29T09:54:07.234318Z","shell.execute_reply.started":"2025-07-29T09:54:07.227214Z","shell.execute_reply":"2025-07-29T09:54:07.233185Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataset.TensorDataset at 0x7b7be87ee850>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Step 4: Create DataLoader for batching\ntrainloader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:54:28.880510Z","iopub.execute_input":"2025-07-29T09:54:28.881406Z","iopub.status.idle":"2025-07-29T09:54:28.886692Z","shell.execute_reply.started":"2025-07-29T09:54:28.881376Z","shell.execute_reply":"2025-07-29T09:54:28.885505Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"##  Step 2: Define the Neural Network Model\n\nNow that data is ready, we will define a basic feedforward neural network using PyTorch.\n\nThis model will:\n- Flatten the 28×28 input into a 784-length vector\n- Pass it through two hidden layers with ReLU activations\n- Output 10 values (logits), one for each digit class (0–9)\n\nwe will use `torch.nn.Module`, which allows to organize the layers and define the forward pass.\n","metadata":{}},{"cell_type":"markdown","source":"## My Explanation of the Model Architecture\n\nBefore training the model, it is imperative to undestand how it works\n---\n\n###  What is a Feedforward Neural Network?\n\nA feedforward neural network is the most basic type of neural network. Data flows in one direction — from the input layer, through the hidden layers, and finally to the output layer. There are no loops or memory. This makes it simple and good for tasks like image classification.\n\n\n### What is `nn.Linear`?\n\n`nn.Linear` is a fully connected (dense) layer. It takes an input, multiplies it by a weight matrix, adds a bias, and outputs a result. The input and output sizes are defined by us. \n\nFor example, `nn.Linear(784, 128)` means we’re taking a 784-dimensional input (a flattened 28x28 image) and mapping it to a 128-dimensional output. This helps the model learn important features from the image.\n\n\n###  What is `nn.ReLU()`?\n\nReLU stands for Rectified Linear Unit. It’s an activation function that keeps only the positive values from the previous layer (i.e., it sets all negative values to zero). This helps the network learn complex, non-linear relationships in the data. It also makes training faster and avoids problems like vanishing gradients.\n\n\n### Putting it Together\n\nIn the model, we are  stacking multiple `Linear` layers with `ReLU` activations in between:\n\n- Input (784) → Linear → ReLU\n- → Linear → ReLU\n- → Linear → Output (10 classes for digits 0–9)\n\nThis setup gives the model the ability to learn useful patterns from the image data and make accurate predictions.\n","metadata":{}},{"cell_type":"code","source":"\nimport torch.nn as nn\n\n\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n\n        # Define layers inside a Sequential block\n        self.network = nn.Sequential(\n            nn.Linear(784, 128),  # Input layer to first hidden layer\n            nn.ReLU(),            # Activation function\n            nn.Linear(128, 64),   # Second hidden layer\n            nn.ReLU(),            # Activation\n            nn.Linear(64, 10)     # Output layer (10 classes)\n        )\n\n    def forward(self, x):\n        # Forward pass through the network\n        return self.network(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T09:58:08.735355Z","iopub.execute_input":"2025-07-29T09:58:08.736678Z","iopub.status.idle":"2025-07-29T09:58:08.743389Z","shell.execute_reply.started":"2025-07-29T09:58:08.736600Z","shell.execute_reply":"2025-07-29T09:58:08.742108Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Step 3: Set Up the Loss Function, Optimizer, and Device\n\nBefore we can train the model, we need to define three important components:\n\n### Loss Function\n\nThe loss function measures how far the model's predictions are from the actual labels.\n\nWe'll use `CrossEntropyLoss`, which is ideal for multi-class classification. It combines LogSoftmax and Negative Log Likelihood into one function. This loss expects the model to output raw scores (logits) ,not probabilities.\n\n\n###  Optimizer\n\nThe optimizer is responsible for updating the model’s weights based on the computed loss. We'll use `Adam`, which is an adaptive optimizer. It adjusts the learning rate for each parameter and generally leads to faster and more stable training than traditional optimizers like SGD.\n\n### Device (CPU or GPU)\n\nIf a GPU is available, we can use it to speed up training significantly. We'll check if CUDA is available and move our model to the appropriate device accordingly.\n","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move the model to the selected device\nmodel = SimpleNN().to(device)\n\n# Define the loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define the optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:07:52.862030Z","iopub.execute_input":"2025-07-29T10:07:52.862406Z","iopub.status.idle":"2025-07-29T10:07:52.879389Z","shell.execute_reply.started":"2025-07-29T10:07:52.862381Z","shell.execute_reply":"2025-07-29T10:07:52.878330Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"##  Step 4: Train the Model (Epoch Loop)\n\nNow that we’ve set up our model, loss function, optimizer, and device, it's time to train the model.\n\n\n### 🔄 What happens during training?\n\nFor each **epoch** ( a full pass through the training dataset), we:\n\n1. Loop over batches from the DataLoader\n2. Move inputs and labels to the selected device (CPU/GPU)\n3. Perform a **forward pass** to get predictions from the model\n4. Compute the **loss** between predictions and actual labels\n5. Perform a **backward pass** to calculate gradients\n6. Use the **optimizer** to update the model’s weights\n\nWe’ll also track the average loss per epoch to see how training is progressing.\n","metadata":{}},{"cell_type":"code","source":"# Re-initialize model if needed\nmodel = SimpleNN().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Metrics storage\nloss_history = []\naccuracy_history = []\n\nepochs = 5\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in trainloader:\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss\n        running_loss += loss.item()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    # Calculate average loss and accuracy for the epoch\n    epoch_loss = running_loss / len(trainloader)\n    epoch_acc = 100 * correct / total\n\n    loss_history.append(epoch_loss)\n    accuracy_history.append(epoch_acc)\n\n    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:17:52.874106Z","iopub.execute_input":"2025-07-29T10:17:52.874458Z","iopub.status.idle":"2025-07-29T10:18:12.350295Z","shell.execute_reply.started":"2025-07-29T10:17:52.874434Z","shell.execute_reply":"2025-07-29T10:18:12.349065Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5 | Loss: 1.4404 | Accuracy: 51.15%\nEpoch 2/5 | Loss: 0.6958 | Accuracy: 77.55%\nEpoch 3/5 | Loss: 0.5495 | Accuracy: 83.06%\nEpoch 4/5 | Loss: 0.4796 | Accuracy: 85.56%\nEpoch 5/5 | Loss: 0.4348 | Accuracy: 87.08%\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"##  Step 5: Evaluate the Model on the Test Set\n\nAfter training, we need to evaluate how well our model performs on data it has **never seen before** — the test set.\n\n###  Why evaluate?\n\nTraining accuracy alone doesn’t tell us if the model has learned to generalize. By testing on unseen data, we check whether the model can make correct predictions outside the training distribution.\n\n### How we'll do it:\n\n1. Loop through batches of the test data\n2. Use the trained model to predict labels\n3. Compare predictions with actual labels\n4. Calculate overall accuracy\n\nWe’ll also make sure:\n- We disable gradient tracking (`torch.no_grad()`) since we’re not training\n- The model is in **evaluation mode** using `model.eval()`\n","metadata":{}},{"cell_type":"code","source":"# Load test.csv and normalize pixel values\ntest_df = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\ntest_images = test_df.values / 255.0  # Normalize\n\n# Convert to PyTorch tensor\nX_test = torch.tensor(test_images, dtype=torch.float32)\n\n# Create DataLoader\ntestloader = DataLoader(X_test, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:15:23.772445Z","iopub.execute_input":"2025-07-29T10:15:23.772942Z","iopub.status.idle":"2025-07-29T10:15:25.694819Z","shell.execute_reply.started":"2025-07-29T10:15:23.772912Z","shell.execute_reply":"2025-07-29T10:15:25.693441Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Load test.csv and normalize pixel values\ntest_df = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\ntest_images = test_df.values / 255.0  # Normalize\n\n# Convert to PyTorch tensor\nX_test = torch.tensor(test_images, dtype=torch.float32)\n\n# Create DataLoader\ntestloader = DataLoader(X_test, batch_size=64, shuffle=False)\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Store predictions\nall_preds = []\n\nwith torch.no_grad():\n    for batch in testloader:\n        batch = batch.to(device)\n        outputs = model(batch)\n        _, preds = torch.max(outputs, 1)\n        all_preds.extend(preds.cpu().numpy())  # Move back to CPU\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    \"ImageId\": list(range(1, len(all_preds) + 1)),\n    \"Label\": all_preds\n})\n\n# Save to CSV (Kaggle-compatible)\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T10:15:46.095311Z","iopub.execute_input":"2025-07-29T10:15:46.095718Z","iopub.status.idle":"2025-07-29T10:15:48.702337Z","shell.execute_reply.started":"2025-07-29T10:15:46.095690Z","shell.execute_reply":"2025-07-29T10:15:48.701205Z"}},"outputs":[{"name":"stdout","text":"✅ submission.csv generated!\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}